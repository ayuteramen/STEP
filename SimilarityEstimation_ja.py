# -*- coding: utf-8 -*-
"""0520281B_exp5_3_ja_寺面杏優.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HGiMS7Ao2AVMZVFssVqxnr6DcELvvZPx
"""

from google.colab import drive
drive.mount('/content/drive')

fin1=open('/content/drive/MyDrive/text/similarity (english)/sample.label.txt')
sample_label=fin1.read()
fin1.close()
fin2=open('/content/drive/MyDrive/text/similarity (english)/sample.text.txt')
sample_text=fin2.read()
fin2.close()
fin3=open('/content/drive/MyDrive/text/similarity (english)/test.text.txt')
test_text=fin3.read()
fin3.close()

# インストール

! pip install sudachipy
! pip install sudachidict_core

! pip install gensim

! curl -Lo chive-1.2-mc5_gensim.tar.gz https://sudachi.s3-ap-northeast-1.amazonaws.com/chive/chive-1.2-mc5_gensim.tar.gz
! tar -zxvf chive-1.2-mc5_gensim.tar.gz

# 単語ベクトルの読み込み

import gensim

w2v = gensim.models.KeyedVectors.load("./chive-1.2-mc5_gensim/chive-1.2-mc5.kv")

mozi = []
mozi.extend(["、", "。"])
# for i in range(12353, 12439):
#     mozi.append(chr(i))
# for i in range(12449, 12532+1):
#     mozi.append(chr(i))
# for i in range(48, 48+10):
#     mozi.append(chr(i))

import re
h = re.compile('^[\u3040-\u309F]+$')

from sudachipy import Dictionary
from sudachipy import SplitMode
tokenizer = Dictionary().create()
import numpy as np

ans_1 = []

def tokenize(text):
    before = list()
    after = list()
    for token in tokenizer.tokenize(text, SplitMode.A):
        before.append(token.surface())
        after.append(token.normalized_form())
    return after

with open('/content/drive/MyDrive/text/similarity (japanese)/test.text.txt', 'r') as f:
    file_data = f.readlines()

    for line in file_data:
        s = line.split('\t')
        print("s = ", s)
        s1 = s[0]
        print("s1 = ", s1)
        s2 = s[1]
        print("s2 = ", s2)
        word_s1 = tokenize(s1)
        word_s2 = tokenize(s2)
        print("word = ", word_s1, word_s2)

        words1 = []
        for i in word_s1:
            if i not in mozi and h.fullmatch(i) == None and i not in words1:
                words1.append(i)
        words2 = []
        for i in word_s2:
            if i not in mozi and h.fullmatch(i) == None and i not in words2:
                words2.append(i)
        print("word = ", words1, words2)

        def vectorize(words, w2v):
            vectors = list()
            for word in words:
                if word in w2v:
                    vectors.append(w2v[word])
            return np.array(vectors).mean(axis=0)   # 平均したベクトル   

        vector1 = vectorize(words1, w2v)
        vector2 = vectorize(words2, w2v)
        
         
        # print("vector = ", vector1, vector2)

        def cos(v1, v2):
            return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))        
        
        print("%1.3f\t%s\t%s" % (cos(vector1, vector2), s1, s2))
        print("\n")

        ans01 = cos(vector1, vector2)
        ans_1.append(ans01)

# # BoW

# ans_2 = []

# def tokenize(text):
#     before = list()
#     after = list()
#     for token in tokenizer.tokenize(text, SplitMode.A):
#         before.append(token.surface())
#         after.append(token.normalized_form())
#     return after

# with open('/content/drive/MyDrive/text/similarity (japanese)/test.text.txt', 'r') as f:
#     file_data = f.readlines()

#     for line in file_data:
#         s = line.split('\t')
#         print("s = ", s)
#         s1 = s[0]
#         print("s1 = ", s1)
#         s2 = s[1]
#         print("s2 = ", s2)
#         word_s1 = tokenize(s1)
#         word_s2 = tokenize(s2)
#         print("word = ", word_s1, word_s2)      

#         words1 = []
#         for i in word_s1:
#             if i not in mozi and h.fullmatch(i) == None and i not in words1:
#                 words1.append(i)
#         words2 = []
#         for i in word_s2:
#             if i not in mozi and h.fullmatch(i) == None and i not in words2:
#                 words2.append(i)
#         print("word = ", words1, words2)

#         # words1 = set(words1)
#         # words2 = set(words2)

#         # 語彙（扱いたい全ての単語）
#         vocab = set()
#         for words in [words1, words2]:
#             for word in words:
#                 vocab.add(word)

#         # 各単語にIDを割り当てる
#         word2id = dict()
#         for word in vocab:
#             word2id[word] = len(word2id)

#         def vectorize(words, vocab, word2id):
#             vector = np.zeros(len(vocab))
#             for word in words:
#                 if word in vocab:
#                     vector[word2id[word]] = 1
#             return vector

#         vector1 = vectorize(words1, vocab, word2id)
#         vector2 = vectorize(words2, vocab, word2id)

#         print("vector = ", vector1, vector2)

#         def cos(v1, v2):
#             return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))        
        
#         print("%1.3f\t%s\t%s" % (cos(vector1, vector2), s1, s2))
#         print("\n")

#         ans01 = cos(vector1, vector2)
#         ans_2.append(ans01)

def tokenize(text):
    before = list()
    after = list()
    for token in tokenizer.tokenize(text, SplitMode.A):
        before.append(token.surface())
        after.append(token.normalized_form())
    return after

with open('/content/drive/MyDrive/text/similarity (japanese)/test.text.txt', 'r') as f:
    file_data = f.readlines()
    
    s = []
    s1 = []
    s2 = []
    for line in file_data:      
        s = re.split('[\t\n]', line)
        # print("s = ", s)
        s1.append(s[0])
        s2.append(s[1])
print("s1 = ", s1)
print("s2 = ", s2)

word_s1 = []
word_s2 = []

for i in range(len(s1)):
    word_s1.append(tokenize(s1[i]))
    word_s2.append(tokenize(s2[i]))
print("word_s1 = ", word_s1)
print("word_s2 = ", word_s2)

word_n1 = []
word_n2 = []
dict_list = []
for i in word_s1:
    w_n1 = []   
    for j in i:
        if j not in mozi and h.fullmatch(j) == None:
            dict_list.append(j)
            if j not in w_n1:
                w_n1.append(j)
    word_n1.append(w_n1)

for i in word_s2:
    w_n2 = []    
    for j in i:
        if j not in mozi and h.fullmatch(j) == None:
            dict_list.append(j)     
            if j not in w_n2:
                w_n2.append(j)
    word_n2.append(w_n2)
   
print("word_n1 = ", word_n1)
print("word_n2 = ", word_n2)
print("dict_list = ", dict_list)

import collections
w2p = collections.Counter(dict_list)

def avg_embedding(word2vec, word2prob):
    vector = np.zeros(word2vec.vector_size)
    for word in word2vec.vocab:
        vector += word2prob[word] * word2vec[word] / len(dict_list)
    return vector

avg_w2v = avg_embedding(w2v, w2p)

def vectorize(words, w2v):
            vectors = list()
            for word in words:
                if word in w2v:
                    vectors.append(w2v[word])
            return np.array(vectors).mean(axis=0)   # 平均したベクトル

def cos(v1, v2):
            return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

Ans = []

for i in range(len(word_n1)):
    vector1 = vectorize(word_n1[i], w2v) - avg_w2v
    vector2 = vectorize(word_n2[i], w2v) - avg_w2v

    ans = cos(vector1, vector2)
    Ans.append(ans)

f = open('answer_J1.txt', 'w')
for a in Ans:
    print(a)
    ans = str(a)
    f.write(ans + "\n")
f.close()

f = open('answer_J2.txt', 'w')
for a, b in zip(ans_1, Ans):
    ans = a * 0.5 + b * 0.5
    print(ans)
    ans = str(ans)
    f.write(ans + "\n")
f.close()