# -*- coding: utf-8 -*-
"""0520281B_exp5_3_en_寺面杏優.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yXuvpEMuNSpB4Z4KfHfh-WVGS6bDvhRS
"""

from google.colab import drive
drive.mount('/content/drive')

# # 単語ベクトルの読み込み

# import gensim.downloader as api

# # en_w2v = api.load("glove-wiki-gigaword-50")   # ここら辺いじるのもあり
# en_w2v = api.load("word2vec-google-news-300")

!unzip -d out /content/drive/MyDrive/text/crawl-300d-2M.vec.zip

import gensim
en_w2v = gensim.models.KeyedVectors.load_word2vec_format('/content/out/crawl-300d-2M.vec', binary=False)

! pip install nltk

import nltk
from nltk.corpus import stopwords
nltk.download("punkt")
nltk.download("averaged_perceptron_tagger")

# nltk.download('stopwords')
# stop_words = stopwords.words('english')
# print(stop_words)

!pip install stop_words
from stop_words import get_stop_words
stop_words = get_stop_words('english')
# stop_words

mozi = []
# for i in range(65, 65+26):
#     mozi.append(chr(i))
# for i in range(97, 97+26):
#     mozi.append(chr(i))
# for i in range(48, 48+10):
#     mozi.append(chr(i))
mozi.extend([",", "."])
print(mozi)

# import re
# def hasNumber(text):
#     if (re.search('\d', text) == None):
#         return False
#     else:
#         return True

import numpy as np

from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk import pos_tag

ans_1 = []

with open('/content/drive/MyDrive/text/similarity (english)/test.text.txt', 'r') as f:
    file_data = f.readlines()
    
    for line in file_data:
        s = line.split('\t')
        print("s = ", s)
        s1 = s[0]
        print("s1 = ", s1)
        s2 = s[1]
        print("s2 = ", s2)
        
        # トークナイズ、小文字化
        word_s1 = []
        for token in pos_tag(word_tokenize(s1)):
            word, pos = token
            word = word.lower()
            word_s1.append(word)
        print("word_s1 = ", word_s1)
        word_s2 = []
        for token in pos_tag(word_tokenize(s2)):
            word, pos = token
            word = word.lower()
            word_s2.append(word)
        print("word_s2 = ", word_s2)
        
        # ストップワード除去
        words1 = []
        for i in word_s1:
            if i not in stop_words and i not in mozi:
                words1.append(i)
        words2 = []
        for i in word_s2:
            if i not in stop_words and i not in mozi:
                words2.append(i)

        # 計算ができないため、長さが1以下の時はストップワード除去を行わない
        if len(words1) <= 1 or len(words2) <= 1:
            for i in word_s1:
                if i not in mozi:
                    words1.append(i)
            for i in word_s2:
                if i not in mozi:
                    words2.append(i)

        print("word = ", words1, words2)

        def vectorize(words, en_w2v):
            vectors = list()
            for word in words:
                if word in en_w2v:
                    vectors.append(en_w2v[word])
            return np.array(vectors).mean(axis=0)   # 平均したベクトル

        vector1 = vectorize(words1, en_w2v)
        vector2 = vectorize(words2, en_w2v)
        # print("vector = ", vector1, vector2)

        def cos(v1, v2):
            return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))        
        
        print("%1.3f\t%s\t%s" % (cos(vector1, vector2), s1, s2))
        print("\n")

        ans = cos(vector1, vector2)
        ans_1.append(ans)

# BoW

ans_2 = []

with open('/content/drive/MyDrive/text/similarity (english)/test.text.txt', 'r') as f:
    file_data = f.readlines()
    
    for line in file_data:
        s = line.split('\t')
        print("s = ", s)
        s1 = s[0]
        print("s1 = ", s1)
        s2 = s[1]
        print("s2 = ", s2)
        
        word_s1 = []
        for token in pos_tag(word_tokenize(s1)):
            word, pos = token
            word = word.lower()
            word_s1.append(word)
        print("word_s1 = ", word_s1)
        word_s2 = []
        for token in pos_tag(word_tokenize(s2)):
            word, pos = token
            word = word.lower()
            word_s2.append(word)
        print("word_s2 = ", word_s2)

        words1 = []
        for i in word_s1:
            if i not in stop_words and i not in mozi:
                words1.append(i)
        words2 = []
        for i in word_s2:
            if i not in stop_words and i not in mozi:
                words2.append(i)

        if len(words1) <= 1 or len(words2) <= 1:
            for i in word_s1:
                if i not in mozi:
                    words1.append(i)
            for i in word_s2:
                if i not in mozi:
                    words2.append(i)

        words1 = set(words1)
        words2 = set(words2)

        # 語彙（扱いたい全ての単語）
        vocab = words1.union(words2) # 和集合とる
        vocab = set(vocab)

        # 各単語にIDを割り当てる
        word2id = dict()
        for word in vocab:
            word2id[word] = len(word2id)

        def vectorize(words, vocab, word2id):
            vector = np.zeros(len(vocab))
            for word in words:
                if word in vocab:
                    vector[word2id[word]] = 1
            return vector

        vector1 = vectorize(words1, vocab, word2id)
        vector2 = vectorize(words2, vocab, word2id)
        print("vector = ", vector1, vector2)

        def cos(v1, v2):
            return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))        
        
        print("%1.3f\t%s\t%s" % (cos(vector1, vector2), s1, s2))
        print("\n")

        ans = cos(vector1, vector2)
        ans_2.append(ans)

# アンサンブル

f = open('answer_E.txt', 'w')
for i, j in zip(ans_1, ans_2):
    ans = i * 0.7 + j * 0.3
    print(ans)
    ans = str(ans)
    f.write(ans + "\n")
f.close()